<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Business learning - organizational behavior reading</title>
    <link href="/site/2021/12/20/business_ob/"/>
    <url>/site/2021/12/20/business_ob/</url>
    
    <content type="html"><![CDATA[<p><strong>Book name:</strong> Organizational behavior by McShane Glinow </p><p><strong>Chapter one:</strong> Introduction</p><ol><li><p><strong>Organizational behavior:</strong> the study of what people think, feel, and do in and around organizations. </p></li><li><p><strong>Organizational effectiveness:</strong> </p></li></ol><ul><li>The open systems perspective views organizations as complex organisms that “live” within an external environment.</li><li>The high-performance work practices (HPWP) perspective identifies a bundle of systems and structures to leverage workforce potential. employee involvement, job autonomy, developing employee competencies, and performance/skill-based rewards.</li><li>According to the organizational learning perspective, organizational effectiveness depends on the organization’s capacity to acquire, share, use, and store valuable knowledge.</li><li>The stakeholder perspective states that leaders manage the interests of diverse stakeholders by relying on their personal and organizational values for guidance.</li></ul><ol start="3"><li><strong>What new factors bring to the organizations.</strong></li></ol><ul><li>globalization: it has several economic and social benefits, but it may also be responsible for work intensification, reduced job security, and lessening work–life balance.</li><li>workforce diversity: Diversity may be a com- petitive advantage that improves decision making and team performance on complex tasks, but it also imposes numerous challenges, such as dysfunctional team conflict and lower team performance.</li><li>emerging employment relationships: One emerging employment relationship trend is a call for more work–life balance (minimizing conflict between work and nonwork demands). Another employment trend is virtual work, particularly working from home (tele- work).</li></ul><ol start="4"><li><strong>anchors for organizational behavior.</strong></li></ol><ul><li>multidisciplinary: psychology, sociology, economics</li><li>systematic research: OB knowledge should be based on systematic research, consistent with evidence-based management.</li><li>contingency: different consequences in different situations.</li><li>multiple levels of analysis: OB topics may be viewed from the individual, team, and organization levels of analysis.</li></ul>]]></content>
    
    
    <categories>
      
      <category>zoom-out</category>
      
    </categories>
    
    
    <tags>
      
      <tag>reading</tag>
      
      <tag>business</tag>
      
      <tag>behavior</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Business learning - OB chapter 2 - Individual behavior and processes.</title>
    <link href="/site/2021/12/20/business_ob_chapter2/"/>
    <url>/site/2021/12/20/business_ob_chapter2/</url>
    
    <content type="html"><![CDATA[<p><strong>Book name:</strong> Organizational behavior by McShane Glinow </p><p><strong>Chapter 2:</strong> individual behavior and processes</p><ol><li><p><strong>Individual performance formula:</strong> </p><p>MARS model: performance = motivation * ability * role perceptions * situation factors</p></li><li><p><strong>Types of individual behavior:</strong> </p></li></ol><ul><li><strong>Task performance:</strong> proficiency, adaptibility, proactivity.</li><li><strong>Organizational citizenship behaviors (OCBs)</strong>: Some OCBs are directed toward individuals, such as assisting coworkers with their work problems, adjusting work schedules to accommodate coworkers, showing genuine courtesy toward coworkers, and sharing work resources (supplies, technology, staff ) with coworkers. Other OCBs represent cooperation and helpfulness toward the organization, such as sup- porting the company’s public image, taking discretionary action to help the organization avoid potential problems, offering ideas beyond those required for their own job, attending voluntary functions that support the organization, and keeping up with new developments in the organization.</li><li><strong>Counterproductive work behaviors (CWBs):</strong> CWBs are voluntary behaviors that have the potential to directly or indirectly harm the organization.24 Some of the many types of CWBs include harassing coworkers, creating unnecessary con- flict, deviating from preferred work methods (e.g., shortcuts that risk work quality), being untruthful, stealing, sabotaging work, avoiding work obligation (tardiness), and wasting re- sources.</li><li><strong>Joining and staying with the organization:</strong> Organizations are people working together toward common goals, so hiring and retaining talent is another critical set of behaviors.</li><li><strong>Maintaining work attendance:</strong> organizations need everyone to show up for work at scheduled times</li></ul><ol start="3"><li>Personality types: Five factor model, CANOE</li></ol><ul><li><strong>Conscientiousness</strong> characterizes people who are organized, dependable, goal-focused, thorough, disciplined, methodical, and industrious. People with low conscientiousness tend to be careless, disorganized, and less thorough.</li><li><strong><em>Agreeableness</em>.</strong> This dimension includes the traits of being trusting, helpful, good- natured, considerate, tolerant, selfless, generous, and flexible. People with low agree- ableness tend to be uncooperative and intolerant of others’ needs as well as more suspicious and self-focused.</li><li><strong>Neuroticism</strong> characterizes people who tend to be anxious, insecure, self-conscious, depressed, and temperamental. In contrast, people with low neuroti- cism (high emotional stability) are poised, secure, and calm.</li><li><strong>Openness to experience</strong> is the most complex and has the least agreement among scholars. It generally refers to the extent to which people are imaginative, creative, unconventional, curious, nonconforming, autonomous, and aesthetically perceptive. Those who score low on this dimension tend to be more resistant to change, less open to new ideas, and more conventional and fixed in their ways.</li><li><strong>Extraversion</strong> characterizes people who are outgoing, talkative, ener- getic, sociable, and assertive. The opposite is <em>introversion,</em> which characterizes those who are quiet, cautious, and less interactive with others. Extraverts get their energy from the outer world (people and things around them), whereas introverts get their energy from the internal world, such as personal reflection on concepts and ideas. Introverts do not necessarily lack social skills. Rather, they are more inclined to direct their interests to ideas than to social events. Introverts feel quite comfortable being alone, whereas extraverts do not.</li></ul><ol start="4"><li><strong>Individual values:</strong></li></ol><p><img src="/site/img/ob_values.png"></p>]]></content>
    
    
    <categories>
      
      <category>zoom-out</category>
      
    </categories>
    
    
    <tags>
      
      <tag>reading</tag>
      
      <tag>business</tag>
      
      <tag>behavior</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Graph Neural Nets Paper List</title>
    <link href="/site/2021/11/07/paper_graph2/"/>
    <url>/site/2021/11/07/paper_graph2/</url>
    
    <content type="html"><![CDATA[<p>Fundamental papers to read in graph neural nets:</p><ul><li>node2vec</li><li>line</li><li>sdne</li><li>metapath2vec</li><li>transe</li><li>gat</li><li>graphsage</li><li>gcn</li><li>ggnn</li><li>mpnn</li><li>hgt</li></ul>]]></content>
    
    
    <categories>
      
      <category>gnn</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper-reading</tag>
      
      <tag>gnn</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Natual Language Processing Paper List</title>
    <link href="/site/2021/11/07/paper_nlp2/"/>
    <url>/site/2021/11/07/paper_nlp2/</url>
    
    <content type="html"><![CDATA[<p>Fundamental papers to read in each applications in NLP:</p><ul><li><p>baseline</p><ul><li>Word2Vec</li><li>glove</li><li>char embedding</li><li>textcnn</li><li>chartextcnn</li><li>fasttext</li><li>deep_nmt</li><li>attention_nmt</li><li>han_attention</li><li>sequence generation model - sgm</li></ul></li><li><p>information extraction - named entity recognition</p><ul><li>BiLSTM-CRF</li><li>LatticeLSTM</li><li>LR-CNN</li><li>LGN</li><li>TENER</li><li>Soft-Lexicon</li></ul></li><li><p>information extraction - relationship extraction</p><ul><li>CNN for re</li><li>pcnn-crcnn</li><li>pcnn-code</li><li>att-blstm</li><li>lstm-lstm-bias</li><li>casrel</li></ul></li><li><p>Pretrained model</p><ul><li>transformer</li><li>transformer-xl</li><li>elmo</li><li>gpt</li><li>bert</li><li>ulmfit</li><li>albert</li><li>mass</li><li>xlnet</li><li>electra</li></ul></li><li><p>semantic text matching</p><ul><li>dssm</li><li>siamesenet</li><li>compare-aggregate</li><li>esim</li><li>BiMPM</li><li>RE2</li><li>MGCN</li><li>MatchPyramid</li><li>semantic aware bert for language understanding</li></ul></li><li><p>translation</p><ul><li>loung-nmt</li><li>coverage</li><li>subword-nmt</li><li>massive exploration of neural machine translation architectures</li></ul></li><li><p>emotional analysis</p><ul><li>tree lstm</li><li>td-lstm &amp; at-lstm</li><li>MemNet&amp;IAN</li><li>Bert&amp;Ernie</li></ul></li><li><p>Reading comprehension</p><ul><li>Teaching machines to read and comprehend</li><li>bidaf</li><li>pgnet</li><li>Improving the robustness of question answering systems to question paraphrasing</li><li>xlnet</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>nlp</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper-reading</tag>
      
      <tag>nlp</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computer Vision Paper List</title>
    <link href="/site/2021/09/28/paper/"/>
    <url>/site/2021/09/28/paper/</url>
    
    <content type="html"><![CDATA[<p>Fundamental papers to read in each applications in computer vision:</p><ul><li><p>baseline</p><ul><li>Alexnet</li><li>VGG</li><li>GoogLeNet v1-v4</li><li>ResNet</li><li>ResNeXt</li><li>DenseNet</li><li>SEnet</li></ul></li><li><p>image segmentation</p><ul><li>FCN</li><li>Unet</li><li>SegNet</li><li>DeepLab</li><li>GCN</li><li>DFN</li><li>ENet</li><li>BiSeNet</li><li>DFANet</li><li>RedNet</li><li>RDFNet</li></ul></li><li><p>object detection</p><ul><li>YOLO</li><li>SSD</li><li>FasterRCNN</li><li>FPN</li><li>RETINANet</li><li>MaskRCNN</li><li>Fcos</li><li>EfficientNet</li><li>Cascade</li><li>CenterNet</li></ul></li><li><p>Gan</p><ul><li>GAN</li><li>cGAN</li><li>DCGAN</li><li>ITGAN</li><li>pix2pix</li><li>cycleGan</li><li>ProGan</li><li>StackGan</li><li>BigGan</li><li>StyleGan</li></ul></li><li><p>ocr</p><ul><li>CRNN</li><li>attention_OCR</li><li>CTPN</li><li>EAST</li><li>Adv_EAST</li><li>PSE</li><li>PAN</li><li>DB</li></ul></li><li><p>light-weight network</p><ul><li>MobileNet</li><li>ShuffleNet</li><li>SqueezeNet</li><li>xception</li><li>Knowledge Disdilation</li><li>attention-transfer</li><li>Learning wights and connections</li><li>Network Slimming</li><li>Prunning for efficient inference</li></ul></li><li><p>cv transformer</p></li><li><p>face recognition</p><ul><li>eigenFaces for recognition</li><li>FaceNet</li><li>DeepId</li><li>A discriminative feature learning Approach for Deep Face Recognition</li><li>Large-Margin softmax loss for CNN</li><li>Sphere-Face</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>cv</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper-reading</tag>
      
      <tag>cv</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>heap</title>
    <link href="/site/2021/09/27/heap/"/>
    <url>/site/2021/09/27/heap/</url>
    
    <content type="html"><![CDATA[<p>Heap is an intricate data structure. By maintaining a binary tree storing structure in the array (without external strorage), it is widely used in many cases like sorting, top elements, and others. Here are important functions of it as memo. All the implementations are from Python source code, which works on a min heap. Siftdown and siftup operations are inverse compared to max heap which are mostly adopted in textbooks.</p><ol><li>push a new item into a min heap.(heapq.heappush)</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Add a new item from pos to startpos in order to find a proper position to maintain the heap structure.</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_siftdown</span>(<span class="hljs-params">h, startpos, pos</span>):</span><br>  newitem = h[pos]<br><br>  <span class="hljs-keyword">while</span> pos &gt; startpos:<br>    <span class="hljs-comment"># Find the parent postion by dividing 2.</span><br>    parentpos = (pos - <span class="hljs-number">1</span>) &gt;&gt; <span class="hljs-number">2</span><br>    <span class="hljs-comment"># If the new parent is larger, move the new item up.</span><br>    <span class="hljs-keyword">if</span> h[parentpos] &gt; newitem:<br>      h[pos] = h[parentpos]<br>      pos = parentpos<br><br>      <span class="hljs-keyword">continue</span><br>     <span class="hljs-comment"># If the parent is not larger, we have found the place.</span><br>     <span class="hljs-keyword">break</span><br>     <span class="hljs-comment"># Set the value.</span><br>     h[pos] = newitem<br>            <br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">heappush</span>(<span class="hljs-params">h, item</span>):</span><br>  h.append(item)<br>  _siftdown(h, <span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(h)-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><ol start="2"><li>pop the min value and adjust.(heapq.heappop)</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_siftup</span>(<span class="hljs-params">h, pos</span>):</span><br>  <span class="hljs-comment"># Move the position to the leaf and siftdown.</span><br>  endpos = <span class="hljs-built_in">len</span>(h)<br>  val = h[pos]<br>  startpos = pos<br>  childpos = pos*<span class="hljs-number">2</span> + <span class="hljs-number">1</span><br>  <span class="hljs-keyword">while</span> childpos &lt; endpos:<br>    rightpos = childpos + <span class="hljs-number">1</span><br><br>    <span class="hljs-comment"># Always swap a smaller value up.</span><br>    <span class="hljs-keyword">if</span> rightpos &lt; endpos <span class="hljs-keyword">and</span> h[rightpos] &lt; h[childpos]:<br>      rightpos, childpos = childpos, rightpos<br>    h[pos] = h[childpos]<br><br>    <span class="hljs-comment"># Update pos to adjust the remaining tree.</span><br>    pos = childpos<br>    childpos = <span class="hljs-number">2</span>*pos + <span class="hljs-number">1</span><br><br>  h[pos] = val<br><br>  <span class="hljs-comment"># Find a place for the starting pos value.</span><br>  _siftdown(h, startpos, pos)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">heappop</span>(<span class="hljs-params">heap</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Pop the smallest item off the heap, maintaining the heap invariant.&quot;&quot;&quot;</span><br>    lastelt = heap.pop()    <span class="hljs-comment"># raises appropriate IndexError if heap is empty</span><br>    <span class="hljs-keyword">if</span> heap:<br>        returnitem = heap[<span class="hljs-number">0</span>]<br>        heap[<span class="hljs-number">0</span>] = lastelt<br>        _siftup(heap, <span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> returnitem<br>    <span class="hljs-keyword">return</span> lastelt<br></code></pre></td></tr></table></figure><ol start="3"><li><p>heapify, O(n)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">heapify</span>(<span class="hljs-params">x</span>):</span><br>  n = <span class="hljs-built_in">len</span>(x)<br>  <span class="hljs-comment"># Transform bottom-up.  The largest index there&#x27;s any point to looking at</span><br>  <span class="hljs-comment"># is the largest with a child index in-range, so must have 2*i + 1 &lt; n,</span><br>  <span class="hljs-comment"># or i &lt; (n-1)/2.  If n is even = 2*j, this is (2*j-1)/2 = j-1/2 so</span><br>  <span class="hljs-comment"># j-1 is the largest, which is n//2 - 1.  If n is odd = 2*j+1, this is</span><br>  <span class="hljs-comment"># (2*j+1-1)/2 = j so j-1 is the largest, and that&#x27;s again n//2-1.</span><br>  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(<span class="hljs-built_in">range</span>(n//<span class="hljs-number">2</span>)):<br>    _siftup(x, i)<br><br></code></pre></td></tr></table></figure></li><li><p>heapsort</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">heapsort</span>(<span class="hljs-params">x</span>):</span><br>  heapify(x)<br>  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(x)//<span class="hljs-number">2</span>)):<br>    _siftup(x, i)<br><br></code></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    <categories>
      
      <category>data structures and algorithm</category>
      
    </categories>
    
    
    <tags>
      
      <tag>data structures and algorithm</tag>
      
      <tag>heap</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Create a trend</title>
    <link href="/site/2021/09/12/technology%20trend/"/>
    <url>/site/2021/09/12/technology%20trend/</url>
    
    <content type="html"><![CDATA[<p>Today, Andrew Ng announced a campaign to have <a href="https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=704aea0974f5">data-centric AI</a> which is inspiring, but I am thinking about what he is doing as a thought leader.</p><p>It is said that following the trend is always smart, but creating a trend is even smarter and respectable. For leaders either in academic field or organizations, one of their main task is to create new concepts, sell them, make them happen, and be responsible for the outcomes. As Jeff Bezos said, leaders have 2 to 3 important decisions to make every day. Leaders also have 2 to 3 trends to create every year. As engineers become more versant as individual contributors, they may also put themselves into leaders’ positions and think in their shoes.</p>]]></content>
    
    
    <categories>
      
      <category>zoom-out</category>
      
    </categories>
    
    
    <tags>
      
      <tag>zoom-out</tag>
      
      <tag>technology</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Different technical roles</title>
    <link href="/site/2021/09/11/technical%20talents/"/>
    <url>/site/2021/09/11/technical%20talents/</url>
    
    <content type="html"><![CDATA[<p>For different team members in a tech organization or even different times a technician is in, there are different roles. Keep it in my mind, it may be useful to career planning and development. From my past work experience, here are role types I can think of:</p><ol><li>Business type. Deep understanding of business, technology trend in mind, systematic planning for future development.</li><li>Platform type. Technology depth and break-throughs.</li><li>Solution type. Technology breadth and being aware of technological cost, operational cost, ROI, and long-term process and mechanism.</li></ol>]]></content>
    
    
    <categories>
      
      <category>zoom-out</category>
      
    </categories>
    
    
    <tags>
      
      <tag>zoom-out</tag>
      
      <tag>technology</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Some thoughts about technology management</title>
    <link href="/site/2021/09/11/technology%20management/"/>
    <url>/site/2021/09/11/technology%20management/</url>
    
    <content type="html"><![CDATA[<h2 id="Sometimes-we-need-to-look-ahead-to-have-a-high-level-perspective-For-programmers-think-beyond-coding"><a href="#Sometimes-we-need-to-look-ahead-to-have-a-high-level-perspective-For-programmers-think-beyond-coding" class="headerlink" title="Sometimes we need to look ahead to have a high-level perspective. For programmers, think beyond coding!"></a>Sometimes we need to look ahead to have a high-level perspective. For programmers, think beyond coding!</h2><p>There goes a famous saying, a soldier who does not want to be a general is not a good soldier. For programmers, I am thinking about what CTOs might focus on. Here is what I can see for now.</p><ol><li>Less code. Legacy code and models can be really problemistic for teams. Not changeable, not moveable, not comprehensive as the team grows.</li><li>Minimum viable product(mvp) is badly needed to get credits and earn more investment and confidence.</li><li>Stability versus innovation. We need to strike a balance of the both. For large or developed companies, stability is the priority. Vice versa for startups.</li><li>Manage talents and technology like a flow. Just like what CFOs are doing with finance flow.</li></ol>]]></content>
    
    
    <categories>
      
      <category>zoom-out</category>
      
    </categories>
    
    
    <tags>
      
      <tag>zoom-out</tag>
      
      <tag>technology</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Paper-reading GAN</title>
    <link href="/site/2021/09/10/GAN/"/>
    <url>/site/2021/09/10/GAN/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> Generative Adversarial Nets</p><p><strong>Task:</strong> Generate images(or other data)</p><p><strong>Abstracts:</strong> </p><ol><li>a new network for estimating generative models.</li><li>generator: capture the data distribution; discriminator: estimates the probability that a sample came from the training data rather than G.</li></ol><p><strong>Related work:</strong></p><p>restricted Boltzmann machines (RBMs)<br>deep Boltzmann machines (DBMs)<br>Deep belief networks (DBNs)<br>noise-contrastive estimation (NCE)<br>generative stochastic network (GSN)</p><p><strong>Key components:</strong><br>Generators:<br>To learn the generator’s distribution pg over data x, we define a prior on input noise variables pz(z), then represent a mapping to data space as G(z; θg), where G is a differentiable function represented by a multilayer perceptron with parameters θg.</p><p>Discriminators:<br>D(x) represents the probability that x came from the data rather than pg. We train D to maximize the probability of assigning the correct label to both training examples and samples from G. We simultaneously train G to minimize log(1 − D(G(z)))</p><p>Theoretical proofs:</p><ol><li>Global Optimality of pg = pdata</li><li>Convergence of the algorithm</li></ol><p>Evaluation method:</p><ol><li>We estimate probability of the test set data under pg by fitting a Gaussian Parzen window to the samples generated with G and reporting the log-likelihood under this distribution. </li></ol><p><img src="/site/img/gan-eval.png"></p><ol start="2"><li>Use interpolating values to verify the distribution is properly learned.</li></ol><p><img src="/site/img/gan-inter.png"></p><p><strong>Future work:</strong></p><p>This framework admits many straightforward extensions:</p><ol><li>A conditional generative model p(x | c) can be obtained by adding c as input to both G and D.</li><li>One can approximately model all conditionals p(xS | x̸S) where S is a subset of the indices of x by training a family of conditional models that share parameters.</li><li>Semi-supervised learning: features from the discriminator or inference net could improve performance of classifiers when limited labeled data is available.</li><li>Efficiency improvements: training could be accelerated greatly by divising better methods for coordinating G and D or determining better distributions to sample z from during training.</li></ol>]]></content>
    
    
    <categories>
      
      <category>cv</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper-reading</tag>
      
      <tag>cv</tag>
      
      <tag>gan</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Don&#39;t be carried away</title>
    <link href="/site/2021/07/25/Don&#39;t%20be%20carried%20away/"/>
    <url>/site/2021/07/25/Don&#39;t%20be%20carried%20away/</url>
    
    <content type="html"><![CDATA[<p>Yesterday on some basketball court in Beijing, two players were applying great defending pressure to each other. Tensions arose. One of the couple finally <strong>couldn’t contain himself</strong> and had some flagrant foul which is hard to see on the play ground where every one is to <strong>exercise and entertain</strong>. </p><p>It occured to me that in “Godfather I” when the godfather had to find some soldiers to complete a task, he said to his consigliere, “find someone calm and reliable, who would <strong>not be carried away by others.</strong>“</p><p><strong>Being stable and calm</strong> is not easy especially when the environment is dragging you into <strong>negative emotions</strong>. We need to keep it in mind that compared to long-term interest like happiness and health, self-esteem is the least important. But <strong>stableness is not silence</strong>. When you are confronted and challenged publicly, respond with <strong>reasoning and logic</strong> without emotion. <strong>Silence is not gold</strong>.</p>]]></content>
    
    
    <categories>
      
      <category>zoom-out</category>
      
    </categories>
    
    
    <tags>
      
      <tag>life</tag>
      
      <tag>thoughts</tag>
      
      <tag>rules</tag>
      
      <tag>zoom-out</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Paper-reading DQN</title>
    <link href="/site/2021/07/07/DQN/"/>
    <url>/site/2021/07/07/DQN/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> DQN </p><p><strong>Task:</strong> Play video games</p><p><strong>Challenges:</strong> </p><ol><li>Lack of detail and low-quality problem for high resolution images.</li><li>To get high-resolution images, there are too many up-sampling layers which make unstable training and meaningless output.</li></ol><p><strong>Existing methods:</strong></p><ol><li>VAE</li><li>PixelRNN</li><li>Energy-based GAN</li><li>Super-resolution methods</li></ol><p><strong>Their innovations:</strong></p><ol><li><p>Image part:</p><p>Preprocessing: gray-scale(avg of 3 RGB channels)</p><p>Down-sampling: 210x160 to 110x84</p><p>Cropping: Get center game area 84x84</p><p>Game part:</p><p>Key frame: only obtain frames periodically(4 or 3 steps for 1)</p><p>Appy 4 frames as the input of Q-network to get historical info.</p><p>Reward clipping: only 3 rewards: +1, -1, 0</p><p>Reasons:</p><p>Image part: common operations.</p><p>Game part: </p><p>Key frame: 1. save time 2. reduce noise from tight actions 3. reduce the duration from action to reward signal.</p><p>4 frames: more context.</p><p>Reward clipping: despite the loss of info, it guarantees the same reward scale for all games.</p></li><li><p>Replay buffer</p><p>definition: a queue to store past transitions for sampling during training</p><p>How to use: proper size. if too little, training data distribution may change instantly; if too large, reward signal may be too sparse.</p><p>Reasons: 1. SGD requires independent samples, but neighboring transitions are highly relative. 2. a pool of transitions make the training more stable. 3. for offline learning, it reduces the variance of updates. 4. multiple sampling allowed to increase data efficiency.</p></li></ol><p><strong>Implementations:</strong></p><p>TODO</p>]]></content>
    
    
    <categories>
      
      <category>Reinforcement Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
      <tag>paper-reading</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Compression</title>
    <link href="/site/2021/07/07/Deep%20Compression/"/>
    <url>/site/2021/07/07/Deep%20Compression/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> Deep Compression</p><p><strong>Task:</strong> Model compression for edge deployment</p><p><strong>Abstracts:</strong> </p><ol><li><p>To deploy deep neural networks on embedded devices, it is vital to reduce the computational complexity and storage size. </p></li><li><p>They proposed deep compression to shrink model size by 35x to 49x without accuracy loss.</p></li><li><p>What it is: a 3-stage pipeline.</p></li></ol><p><img src="/site/img/dcPipeline.png"></p><p><strong>3 Stages</strong></p><ol><li><strong>Pruning</strong></li></ol><p>we start by learning the connectivity via normal network training. Next, we prune the small-weight connections: all connections with weights below a threshold are removed from the network. Finally, we retrain the network to learn the final weights for the remaining sparse connections.</p><p>We store the sparse structure that results from pruning using compressed sparse row (CSR) or compressed sparse column (CSC) format.</p><p>To compress further, we store the index difference instead of the absolute position, and encode this difference in 8 bits for conv layer and 5 bits for fc layer.</p><ol start="2"><li><strong>TRAINED QUANTIZATION AND WEIGHT SHARING</strong></li></ol><p>We limit the number of effective weights we need to store by having multiple connections share the same weight, and then fine-tune those shared weights.</p><p><img src="/site/img/dcQuantization.png"></p><p>We use k-means clustering to identify the shared weights for each layer of a trained network, so that all the weights that fall into the same cluster will share the same weight. Weights are not shared across layers.</p><p>Centroids initialization: We examine three initialization methods: Forgy(random), density-based, and linear initialization.</p><ol start="3"><li><strong>Huffman coding and implementations</strong></li></ol><p>Huffman coding: More common symbols are represented with fewer bits.</p><p>Pruning is implemented by adding a mask to the blobs to mask out the update of the pruned connections. Quantization and weight sharing are implemented by maintaining a codebook structure that stores the shared weight, and group-by-index after calculating the gradient of each layer. Each shared weight is updated with all the gradients that fall into that bucket. Huffman coding doesn’t require training and is implemented offline after all the fine-tuning is finished.</p>]]></content>
    
    
    <categories>
      
      <category>Model Compression</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper-reading</tag>
      
      <tag>model compression</tag>
      
      <tag>edge</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Paper-reading StackGan</title>
    <link href="/site/2021/07/07/StackGan/"/>
    <url>/site/2021/07/07/StackGan/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> StackGan <a href="https://links.jianshu.com/go?to=https://arxiv.org/pdf/1612.03242.pdf">https://arxiv.org/pdf/1612.03242.pdf</a></p><p><strong>Task:</strong> Text to Image</p><p><strong>Challenges:</strong> </p><ol><li>Lack of detail and low-quality problem for high resolution images.</li><li>To get high-resolution images, there are too many up-sampling layers which make unstable training and meaningless output.</li></ol><p><strong>Existing methods:</strong></p><ol><li>VAE</li><li>PixelRNN</li><li>Energy-based GAN</li><li>Super-resolution methods</li></ol><p><strong>Their innovations:</strong></p><ol><li>Two stage GAN: Low-quality images are generated in stage 1 and parameters frozened to feed stage-2 GAN to generate high resolution images.</li><li>Conditional augmentation: To solve training data deficiency, they calculat mean and variance in a FC layer following the word embedding layer to sample conditional variant c. They also apply KL divergence over the above distribution and normal one(N(0,1)) as regulization. It helps generate more training data and after increasing sampling randomness, one sentence can output different images.</li></ol><p><strong>Implementations:</strong></p><p>TODO</p>]]></content>
    
    
    <categories>
      
      <category>cv</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper-reading</tag>
      
      <tag>cv</tag>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Paper-reading transformer</title>
    <link href="/site/2021/07/07/Transformer/"/>
    <url>/site/2021/07/07/Transformer/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> Attention is all you need</p><p><strong>Task:</strong> sequence to sequence</p><p><strong>Abstracts:</strong> </p><ol><li>a new network replacing recurrent network and convolutions with self-attentions only.</li><li>draw global dependencies between input and output.</li><li>more parallelization and can reach a new state of the art in translation quality in no time.</li></ol><p><strong>Model Architechture:</strong></p><ol><li>Encoder-decoder</li><li>Encoder: an input sequence of symbol representations to a sequence of continuous representations</li><li>Decoder: Given z, the decoder then generates an output sequence of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.</li></ol><p><img src="/site/img/transformer-arch.png"></p><ol start="4"><li>Scaled Dot-Product Attention: We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by √dk.</li><li>Multi-head attention: On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv -dimensional output values. These are concatenated and once again projected, resulting in the final values. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</li><li>Position-wise Feed-Forward Networks:Another way of describing this is as two convolutions with kernel size 1</li><li>Positional encodings: To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. </li></ol><p><strong>Other tricks:</strong></p><ol><li>why self-attention:</li></ol><p>One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. </p><ol start="2"><li><p>dropout and label smoothing:</p><p>Dropout=0.1, label smoothing=0.1</p></li></ol><p><strong>Future works:</strong></p><ol><li>Applications in cv.</li><li>Investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs. </li><li>Making generation less sequential.</li></ol>]]></content>
    
    
    <categories>
      
      <category>Natual Language Processing</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper-reading</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Paper-reading textCNN</title>
    <link href="/site/2021/07/07/textCNN/"/>
    <url>/site/2021/07/07/textCNN/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> TextCNN</p><p><strong>Task:</strong> Sentence Classification</p><p><strong>Abstracts:</strong> </p><ol><li>Based on static word embeddings from pretrained models, finetuning with basic CNN models may get good results.</li><li>Finetuning pretrained word embeddings can be useful.</li><li>Making use of both static and task-specific vectors is possible.</li><li>They got SOTAs on 4/7 tasks for sentiment analysis and question classifications.</li></ol><p><strong>Introduction:</strong></p><ol><li>Deep learning</li><li>word vectors</li><li>CNN in NLP(semantic parsing, search query retrieving, and other traditional NLP tasks)</li></ol><p><strong>Model</strong></p><p><img src="/site/img/textcnn1.png"></p><ol><li>k-dimensional word embedding as input(2 channels static and non-static)</li><li>kernel size {3*4*5}*k as different filters, then max pooling(mulple filters to produce multiple feature maps for one word)</li><li>Fc layer with softmax</li></ol><p><strong>Non-static vs static</strong></p><p><img src="/site/img/textcnn2.png"></p><p>Non-static(parameters are updated rather than frozened) is more specific and accurate on tasks.</p><p><strong>Implementations:</strong></p><p>TODO</p>]]></content>
    
    
    <categories>
      
      <category>Natual Language Processing</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper-reading</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/site/2021/06/30/hello-world/"/>
    <url>/site/2021/06/30/hello-world/</url>
    
    <content type="html"><![CDATA[<h2 id="Start-creating-today"><a href="#Start-creating-today" class="headerlink" title="Start creating today!"></a>Start creating today!</h2><p>It’s better late than none.</p><p>The blog name is “Zoom In”, which reminds me to pay attention to the technical details in my career as a machine learning software engineer (for now). I will leave technical notes about artificial intelligence, software engineering and more. In the years to come, I would like to be a <strong>“full-stack” MLE</strong> , skilled in general machine learning, recommendation, computer vision, NLP, reinforcement learning, auto-pilot, and related development skills. Keep it up!!!</p><p>The last but not the least, there will be a “Zoom Out” blog definitely, which focuses on ideas and thoughts in a bigger picture beyond technology. It’s said that the more you get versent in technical skills, the less you think with scope and dimension. Hope it will help me think out of the box as not only an engineer, but also an innovator.</p>]]></content>
    
    
    <categories>
      
      <category>zoom-out</category>
      
    </categories>
    
    
    <tags>
      
      <tag>life</tag>
      
      <tag>zoom-out</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
