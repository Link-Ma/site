<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Paper-reading DQN</title>
    <link href="/site/2021/07/08/DQN/"/>
    <url>/site/2021/07/08/DQN/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> DQN </p><p><strong>Task:</strong> Play video games</p><p><strong>Challenges:</strong> </p><ol><li>Lack of detail and low-quality problem for high resolution images.</li><li>To get high-resolution images, there are too many up-sampling layers which make unstable training and meaningless output.</li></ol><p><strong>Existing methods:</strong></p><ol><li>VAE</li><li>PixelRNN</li><li>Energy-based GAN</li><li>Super-resolution methods</li></ol><p><strong>Their innovations:</strong></p><ol><li><p>Image part:</p><p>Preprocessing: gray-scale(avg of 3 RGB channels)</p><p>Down-sampling: 210x160 to 110x84</p><p>Cropping: Get center game area 84x84</p><p>Game part:</p><p>Key frame: only obtain frames periodically(4 or 3 steps for 1)</p><p>Appy 4 frames as the input of Q-network to get historical info.</p><p>Reward clipping: only 3 rewards: +1, -1, 0</p><p>Reasons:</p><p>Image part: common operations.</p><p>Game part: </p><p>Key frame: 1. save time 2. reduce noise from tight actions 3. reduce the duration from action to reward signal.</p><p>4 frames: more context.</p><p>Reward clipping: despite the loss of info, it guarantees the same reward scale for all games.</p></li><li><p>Replay buffer</p><p>definition: a queue to store past transitions for sampling during training</p><p>How to use: proper size. if too little, training data distribution may change instantly; if too large, reward signal may be too sparse.</p><p>Reasons: 1. SGD requires independent samples, but neighboring transitions are highly relative. 2. a pool of transitions make the training more stable. 3. for offline learning, it reduces the variance of updates. 4. multiple sampling allowed to increase data efficiency.</p></li></ol><p><strong>Implementations:</strong></p><p>TODO</p>]]></content>
    
    
    <categories>
      
      <category>Reinforcement Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
      <tag>paper-reading</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Paper-reading StackGan</title>
    <link href="/site/2021/07/08/StackGan/"/>
    <url>/site/2021/07/08/StackGan/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> StackGan <a href="https://links.jianshu.com/go?to=https://arxiv.org/pdf/1612.03242.pdf">https://arxiv.org/pdf/1612.03242.pdf</a></p><p><strong>Task:</strong> Text to Image</p><p><strong>Challenges:</strong> </p><ol><li>Lack of detail and low-quality problem for high resolution images.</li><li>To get high-resolution images, there are too many up-sampling layers which make unstable training and meaningless output.</li></ol><p><strong>Existing methods:</strong></p><ol><li>VAE</li><li>PixelRNN</li><li>Energy-based GAN</li><li>Super-resolution methods</li></ol><p><strong>Their innovations:</strong></p><ol><li>Two stage GAN: Low-quality images are generated in stage 1 and parameters frozened to feed stage-2 GAN to generate high resolution images.</li><li>Conditional augmentation: To solve training data deficiency, they calculat mean and variance in a FC layer following the word embedding layer to sample conditional variant c. They also apply KL divergence over the above distribution and normal one(N(0,1)) as regulization. It helps generate more training data and after increasing sampling randomness, one sentence can output different images.</li></ol><p><strong>Implementations:</strong></p><p>TODO</p>]]></content>
    
    
    <categories>
      
      <category>cv</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper-reading</tag>
      
      <tag>GAN</tag>
      
      <tag>cv</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Paper-reading textCNN</title>
    <link href="/site/2021/07/08/textCNN/"/>
    <url>/site/2021/07/08/textCNN/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> TextCNN</p><p><strong>Task:</strong> Sentence Classification</p><p><strong>Abstracts:</strong> </p><ol><li>Based on static word embeddings from pretrained models, finetuning with basic CNN models may get good results.</li><li>Finetuning pretrained word embeddings can be useful.</li><li>Making use of both static and task-specific vectors is possible.</li><li>They got SOTAs on 4/7 tasks for sentiment analysis and question classifications.</li></ol><p><strong>Introduction:</strong></p><ol><li>Deep learning</li><li>word vectors</li><li>CNN in NLP(semantic parsing, search query retrieving, and other traditional NLP tasks)</li></ol><p><strong>Model</strong></p><p><img src="/images/textcnn1.png"></p><ol><li>k-dimensional word embedding as input(2 channels static and non-static)</li><li>kernel size {3*4*5}*k as different filters, then max pooling(mulple filters to produce multiple feature maps for one word)</li><li>Fc layer with softmax</li></ol><p><strong>Non-static vs static</strong></p><p><img src="/images/textcnn2.png"></p><p>Non-static(parameters are updated rather than frozened) is more specific and accurate on tasks.</p><p><strong>Implementations:</strong></p><p>TODO</p>]]></content>
    
    
    <categories>
      
      <category>Natual Language Processing</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper-reading</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/site/2021/06/30/hello-world/"/>
    <url>/site/2021/06/30/hello-world/</url>
    
    <content type="html"><![CDATA[<h2 id="Start-creating-today"><a href="#Start-creating-today" class="headerlink" title="Start creating today!"></a>Start creating today!</h2><p>It’s better late than none.</p><p>The blog name is “Zoom In”, which reminds me to pay attention to the technical details in my career as a machine learning software engineer (for now). I will leave technical notes about artificial intelligence, software engineering and more. In the years to come, I would like to be a <strong>“full-stack” MLE</strong> , skilled in general machine learning, recommendation, computer vision, NLP, reinforcement learning, auto-pilot, and related development skills. Keep it up!!!</p><p>The last but not the least, there will be a “Zoom Out” blog definitely, which focuses on ideas and thoughts in a bigger picture beyond technology. It’s said that the more you get versent in technical skills, the less you think with scope and dimension. Hope it will help me think out of the box as not only an engineer, but also an innovator.</p>]]></content>
    
    
    
    <tags>
      
      <tag>life</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
