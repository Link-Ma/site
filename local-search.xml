<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Don&#39;t be carried away</title>
    <link href="/site/2021/07/25/Don&#39;t%20be%20carried%20away/"/>
    <url>/site/2021/07/25/Don&#39;t%20be%20carried%20away/</url>
    
    <content type="html"><![CDATA[<p>Yesterday on some basketball court in Beijing, two players were applying great defending pressure to each other. Tensions arose. One of the couple finally <strong>couldn’t contain himself</strong> and had some flagrant foul which is hard to see on the play ground where every one is to <strong>exercise and entertain</strong>. </p><p>It occured to me that in “Godfather I” when the godfather had to find some soldiers to complete a task, he said to his consigliere, “find someone calm and reliable, who would <strong>not be carried away by others.</strong>“</p><p><strong>Being stable and calm</strong> is not easy especially when the environment is dragging you into <strong>negative emotions</strong>. We need to keep it in mind that compared to long-term interest like happiness and health, self-esteem is the least important. But <strong>stableness is not silence</strong>. When you are confronted and challenged publicly, respond with <strong>reasoning and logic</strong> without emotion. <strong>Silence is not gold</strong>.</p>]]></content>
    
    
    <categories>
      
      <category>thinking</category>
      
    </categories>
    
    
    <tags>
      
      <tag>life</tag>
      
      <tag>thoughts</tag>
      
      <tag>rules</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Compression</title>
    <link href="/site/2021/07/08/Deep%20Compression/"/>
    <url>/site/2021/07/08/Deep%20Compression/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> Deep Compression</p><p><strong>Task:</strong> Model compression for edge deployment</p><p><strong>Abstracts:</strong> </p><ol><li><p>To deploy deep neural networks on embedded devices, it is vital to reduce the computational complexity and storage size. </p></li><li><p>They proposed deep compression to shrink model size by 35x to 49x without accuracy loss.</p></li><li><p>What it is: a 3-stage pipeline.</p></li></ol><p><img src="site/img/dcPipeline.png" alt="img"></p><p><strong>3 Stages</strong></p><ol><li><strong>Pruning</strong></li></ol><p>we start by learning the connectivity via normal network training. Next, we prune the small-weight connections: all connections with weights below a threshold are removed from the network. Finally, we retrain the network to learn the final weights for the remaining sparse connections.</p><p>We store the sparse structure that results from pruning using compressed sparse row (CSR) or compressed sparse column (CSC) format.</p><p>To compress further, we store the index difference instead of the absolute position, and encode this difference in 8 bits for conv layer and 5 bits for fc layer.</p><ol start="2"><li><strong>TRAINED QUANTIZATION AND WEIGHT SHARING</strong></li></ol><p>We limit the number of effective weights we need to store by having multiple connections share the same weight, and then fine-tune those shared weights.</p><p><img src="site/img/dcQuantization.png" alt="img"></p><p>We use k-means clustering to identify the shared weights for each layer of a trained network, so that all the weights that fall into the same cluster will share the same weight. Weights are not shared across layers.</p><p>Centroids initialization: We examine three initialization methods: Forgy(random), density-based, and linear initialization.</p><ol start="3"><li><strong>Huffman coding and implementations</strong></li></ol><p>Huffman coding: More common symbols are represented with fewer bits.</p><p>Pruning is implemented by adding a mask to the blobs to mask out the update of the pruned connections. Quantization and weight sharing are implemented by maintaining a codebook structure that stores the shared weight, and group-by-index after calculating the gradient of each layer. Each shared weight is updated with all the gradients that fall into that bucket. Huffman coding doesn’t require training and is implemented offline after all the fine-tuning is finished.</p>]]></content>
    
    
    <categories>
      
      <category>Model Compression</category>
      
    </categories>
    
    
    <tags>
      
      <tag>model compression</tag>
      
      <tag>paper-reading</tag>
      
      <tag>edge</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Paper-reading DQN</title>
    <link href="/site/2021/07/08/DQN/"/>
    <url>/site/2021/07/08/DQN/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> DQN </p><p><strong>Task:</strong> Play video games</p><p><strong>Challenges:</strong> </p><ol><li>Lack of detail and low-quality problem for high resolution images.</li><li>To get high-resolution images, there are too many up-sampling layers which make unstable training and meaningless output.</li></ol><p><strong>Existing methods:</strong></p><ol><li>VAE</li><li>PixelRNN</li><li>Energy-based GAN</li><li>Super-resolution methods</li></ol><p><strong>Their innovations:</strong></p><ol><li><p>Image part:</p><p>Preprocessing: gray-scale(avg of 3 RGB channels)</p><p>Down-sampling: 210x160 to 110x84</p><p>Cropping: Get center game area 84x84</p><p>Game part:</p><p>Key frame: only obtain frames periodically(4 or 3 steps for 1)</p><p>Appy 4 frames as the input of Q-network to get historical info.</p><p>Reward clipping: only 3 rewards: +1, -1, 0</p><p>Reasons:</p><p>Image part: common operations.</p><p>Game part: </p><p>Key frame: 1. save time 2. reduce noise from tight actions 3. reduce the duration from action to reward signal.</p><p>4 frames: more context.</p><p>Reward clipping: despite the loss of info, it guarantees the same reward scale for all games.</p></li><li><p>Replay buffer</p><p>definition: a queue to store past transitions for sampling during training</p><p>How to use: proper size. if too little, training data distribution may change instantly; if too large, reward signal may be too sparse.</p><p>Reasons: 1. SGD requires independent samples, but neighboring transitions are highly relative. 2. a pool of transitions make the training more stable. 3. for offline learning, it reduces the variance of updates. 4. multiple sampling allowed to increase data efficiency.</p></li></ol><p><strong>Implementations:</strong></p><p>TODO</p>]]></content>
    
    
    <categories>
      
      <category>Reinforcement Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper-reading</tag>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Paper-reading transformer</title>
    <link href="/site/2021/07/08/Transformer/"/>
    <url>/site/2021/07/08/Transformer/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> Attention is all you need</p><p><strong>Task:</strong> sequence to sequence</p><p><strong>Abstracts:</strong> </p><ol><li>a new network replacing recurrent network and convolutions with self-attentions only.</li><li></li></ol>]]></content>
    
    
    <categories>
      
      <category>Natual Language Processing</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper-reading</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Paper-reading StackGan</title>
    <link href="/site/2021/07/08/StackGan/"/>
    <url>/site/2021/07/08/StackGan/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> StackGan <a href="https://links.jianshu.com/go?to=https://arxiv.org/pdf/1612.03242.pdf">https://arxiv.org/pdf/1612.03242.pdf</a></p><p><strong>Task:</strong> Text to Image</p><p><strong>Challenges:</strong> </p><ol><li>Lack of detail and low-quality problem for high resolution images.</li><li>To get high-resolution images, there are too many up-sampling layers which make unstable training and meaningless output.</li></ol><p><strong>Existing methods:</strong></p><ol><li>VAE</li><li>PixelRNN</li><li>Energy-based GAN</li><li>Super-resolution methods</li></ol><p><strong>Their innovations:</strong></p><ol><li>Two stage GAN: Low-quality images are generated in stage 1 and parameters frozened to feed stage-2 GAN to generate high resolution images.</li><li>Conditional augmentation: To solve training data deficiency, they calculat mean and variance in a FC layer following the word embedding layer to sample conditional variant c. They also apply KL divergence over the above distribution and normal one(N(0,1)) as regulization. It helps generate more training data and after increasing sampling randomness, one sentence can output different images.</li></ol><p><strong>Implementations:</strong></p><p>TODO</p>]]></content>
    
    
    <categories>
      
      <category>cv</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper-reading</tag>
      
      <tag>GAN</tag>
      
      <tag>cv</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Paper-reading textCNN</title>
    <link href="/site/2021/07/08/textCNN/"/>
    <url>/site/2021/07/08/textCNN/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> TextCNN</p><p><strong>Task:</strong> Sentence Classification</p><p><strong>Abstracts:</strong> </p><ol><li>Based on static word embeddings from pretrained models, finetuning with basic CNN models may get good results.</li><li>Finetuning pretrained word embeddings can be useful.</li><li>Making use of both static and task-specific vectors is possible.</li><li>They got SOTAs on 4/7 tasks for sentiment analysis and question classifications.</li></ol><p><strong>Introduction:</strong></p><ol><li>Deep learning</li><li>word vectors</li><li>CNN in NLP(semantic parsing, search query retrieving, and other traditional NLP tasks)</li></ol><p><strong>Model</strong></p><p><img src="/site/img/textcnn1.png"></p><ol><li>k-dimensional word embedding as input(2 channels static and non-static)</li><li>kernel size {3*4*5}*k as different filters, then max pooling(mulple filters to produce multiple feature maps for one word)</li><li>Fc layer with softmax</li></ol><p><strong>Non-static vs static</strong></p><p><img src="/site/img/textcnn2.png"></p><p>Non-static(parameters are updated rather than frozened) is more specific and accurate on tasks.</p><p><strong>Implementations:</strong></p><p>TODO</p>]]></content>
    
    
    <categories>
      
      <category>Natual Language Processing</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper-reading</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/site/2021/06/30/hello-world/"/>
    <url>/site/2021/06/30/hello-world/</url>
    
    <content type="html"><![CDATA[<h2 id="Start-creating-today"><a href="#Start-creating-today" class="headerlink" title="Start creating today!"></a>Start creating today!</h2><p>It’s better late than none.</p><p>The blog name is “Zoom In”, which reminds me to pay attention to the technical details in my career as a machine learning software engineer (for now). I will leave technical notes about artificial intelligence, software engineering and more. In the years to come, I would like to be a <strong>“full-stack” MLE</strong> , skilled in general machine learning, recommendation, computer vision, NLP, reinforcement learning, auto-pilot, and related development skills. Keep it up!!!</p><p>The last but not the least, there will be a “Zoom Out” blog definitely, which focuses on ideas and thoughts in a bigger picture beyond technology. It’s said that the more you get versent in technical skills, the less you think with scope and dimension. Hope it will help me think out of the box as not only an engineer, but also an innovator.</p>]]></content>
    
    
    
    <tags>
      
      <tag>life</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
