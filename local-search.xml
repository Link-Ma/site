<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Paper-reading GAN</title>
    <link href="/site/2021/09/10/GAN/"/>
    <url>/site/2021/09/10/GAN/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> Generative Adversarial Nets</p><p><strong>Task:</strong> Generate images(or other data)</p><p><strong>Abstracts:</strong> </p><ol><li>a new network for estimating generative models.</li><li>generator: capture the data distribution; discriminator: estimates the probability that a sample came from the training data rather than G.</li></ol><p><strong>Related work:</strong></p><p>restricted Boltzmann machines (RBMs)<br>deep Boltzmann machines (DBMs)<br>Deep belief networks (DBNs)<br>noise-contrastive estimation (NCE)<br>generative stochastic network (GSN)</p><p><strong>Key components:</strong><br>Generators:<br>To learn the generator’s distribution pg over data x, we define a prior on input noise variables pz(z), then represent a mapping to data space as G(z; θg), where G is a differentiable function represented by a multilayer perceptron with parameters θg.</p><p>Discriminators:<br>D(x) represents the probability that x came from the data rather than pg. We train D to maximize the probability of assigning the correct label to both training examples and samples from G. We simultaneously train G to minimize log(1 − D(G(z)))</p><p>Theoretical proofs:</p><ol><li>Global Optimality of pg = pdata</li><li>Convergence of the algorithm</li></ol><p>Evaluation method:</p><ol><li>We estimate probability of the test set data under pg by fitting a Gaussian Parzen window to the samples generated with G and reporting the log-likelihood under this distribution. </li></ol><p><img src="/site/img/gan-eval.png"></p><ol start="2"><li>Use interpolating values to verify the distribution is properly learned.</li></ol><p><img src="/site/img/gan-inter.png"></p><p><strong>Future work:</strong></p><p>This framework admits many straightforward extensions:</p><ol><li>A conditional generative model p(x | c) can be obtained by adding c as input to both G and D.</li><li>One can approximately model all conditionals p(xS | x̸S) where S is a subset of the indices of x by training a family of conditional models that share parameters.</li><li>Semi-supervised learning: features from the discriminator or inference net could improve performance of classifiers when limited labeled data is available.</li><li>Efficiency improvements: training could be accelerated greatly by divising better methods for coordinating G and D or determining better distributions to sample z from during training.</li></ol>]]></content>
    
    
    <categories>
      
      <category>cv</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper-reading</tag>
      
      <tag>cv</tag>
      
      <tag>gan</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Don&#39;t be carried away</title>
    <link href="/site/2021/07/24/Don&#39;t%20be%20carried%20away/"/>
    <url>/site/2021/07/24/Don&#39;t%20be%20carried%20away/</url>
    
    <content type="html"><![CDATA[<p>Yesterday on some basketball court in Beijing, two players were applying great defending pressure to each other. Tensions arose. One of the couple finally <strong>couldn’t contain himself</strong> and had some flagrant foul which is hard to see on the play ground where every one is to <strong>exercise and entertain</strong>. </p><p>It occured to me that in “Godfather I” when the godfather had to find some soldiers to complete a task, he said to his consigliere, “find someone calm and reliable, who would <strong>not be carried away by others.</strong>“</p><p><strong>Being stable and calm</strong> is not easy especially when the environment is dragging you into <strong>negative emotions</strong>. We need to keep it in mind that compared to long-term interest like happiness and health, self-esteem is the least important. But <strong>stableness is not silence</strong>. When you are confronted and challenged publicly, respond with <strong>reasoning and logic</strong> without emotion. <strong>Silence is not gold</strong>.</p>]]></content>
    
    
    <categories>
      
      <category>thinking</category>
      
    </categories>
    
    
    <tags>
      
      <tag>life</tag>
      
      <tag>thoughts</tag>
      
      <tag>rules</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Compression</title>
    <link href="/site/2021/07/07/Deep%20Compression/"/>
    <url>/site/2021/07/07/Deep%20Compression/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> Deep Compression</p><p><strong>Task:</strong> Model compression for edge deployment</p><p><strong>Abstracts:</strong> </p><ol><li><p>To deploy deep neural networks on embedded devices, it is vital to reduce the computational complexity and storage size. </p></li><li><p>They proposed deep compression to shrink model size by 35x to 49x without accuracy loss.</p></li><li><p>What it is: a 3-stage pipeline.</p></li></ol><p><img src="/site/img/dcPipeline.png"></p><p><strong>3 Stages</strong></p><ol><li><strong>Pruning</strong></li></ol><p>we start by learning the connectivity via normal network training. Next, we prune the small-weight connections: all connections with weights below a threshold are removed from the network. Finally, we retrain the network to learn the final weights for the remaining sparse connections.</p><p>We store the sparse structure that results from pruning using compressed sparse row (CSR) or compressed sparse column (CSC) format.</p><p>To compress further, we store the index difference instead of the absolute position, and encode this difference in 8 bits for conv layer and 5 bits for fc layer.</p><ol start="2"><li><strong>TRAINED QUANTIZATION AND WEIGHT SHARING</strong></li></ol><p>We limit the number of effective weights we need to store by having multiple connections share the same weight, and then fine-tune those shared weights.</p><p><img src="/site/img/dcQuantization.png"></p><p>We use k-means clustering to identify the shared weights for each layer of a trained network, so that all the weights that fall into the same cluster will share the same weight. Weights are not shared across layers.</p><p>Centroids initialization: We examine three initialization methods: Forgy(random), density-based, and linear initialization.</p><ol start="3"><li><strong>Huffman coding and implementations</strong></li></ol><p>Huffman coding: More common symbols are represented with fewer bits.</p><p>Pruning is implemented by adding a mask to the blobs to mask out the update of the pruned connections. Quantization and weight sharing are implemented by maintaining a codebook structure that stores the shared weight, and group-by-index after calculating the gradient of each layer. Each shared weight is updated with all the gradients that fall into that bucket. Huffman coding doesn’t require training and is implemented offline after all the fine-tuning is finished.</p>]]></content>
    
    
    <categories>
      
      <category>Model Compression</category>
      
    </categories>
    
    
    <tags>
      
      <tag>model compression</tag>
      
      <tag>paper-reading</tag>
      
      <tag>edge</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Paper-reading DQN</title>
    <link href="/site/2021/07/07/DQN/"/>
    <url>/site/2021/07/07/DQN/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> DQN </p><p><strong>Task:</strong> Play video games</p><p><strong>Challenges:</strong> </p><ol><li>Lack of detail and low-quality problem for high resolution images.</li><li>To get high-resolution images, there are too many up-sampling layers which make unstable training and meaningless output.</li></ol><p><strong>Existing methods:</strong></p><ol><li>VAE</li><li>PixelRNN</li><li>Energy-based GAN</li><li>Super-resolution methods</li></ol><p><strong>Their innovations:</strong></p><ol><li><p>Image part:</p><p>Preprocessing: gray-scale(avg of 3 RGB channels)</p><p>Down-sampling: 210x160 to 110x84</p><p>Cropping: Get center game area 84x84</p><p>Game part:</p><p>Key frame: only obtain frames periodically(4 or 3 steps for 1)</p><p>Appy 4 frames as the input of Q-network to get historical info.</p><p>Reward clipping: only 3 rewards: +1, -1, 0</p><p>Reasons:</p><p>Image part: common operations.</p><p>Game part: </p><p>Key frame: 1. save time 2. reduce noise from tight actions 3. reduce the duration from action to reward signal.</p><p>4 frames: more context.</p><p>Reward clipping: despite the loss of info, it guarantees the same reward scale for all games.</p></li><li><p>Replay buffer</p><p>definition: a queue to store past transitions for sampling during training</p><p>How to use: proper size. if too little, training data distribution may change instantly; if too large, reward signal may be too sparse.</p><p>Reasons: 1. SGD requires independent samples, but neighboring transitions are highly relative. 2. a pool of transitions make the training more stable. 3. for offline learning, it reduces the variance of updates. 4. multiple sampling allowed to increase data efficiency.</p></li></ol><p><strong>Implementations:</strong></p><p>TODO</p>]]></content>
    
    
    <categories>
      
      <category>Reinforcement Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper-reading</tag>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Paper-reading StackGan</title>
    <link href="/site/2021/07/07/StackGan/"/>
    <url>/site/2021/07/07/StackGan/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> StackGan <a href="https://links.jianshu.com/go?to=https://arxiv.org/pdf/1612.03242.pdf">https://arxiv.org/pdf/1612.03242.pdf</a></p><p><strong>Task:</strong> Text to Image</p><p><strong>Challenges:</strong> </p><ol><li>Lack of detail and low-quality problem for high resolution images.</li><li>To get high-resolution images, there are too many up-sampling layers which make unstable training and meaningless output.</li></ol><p><strong>Existing methods:</strong></p><ol><li>VAE</li><li>PixelRNN</li><li>Energy-based GAN</li><li>Super-resolution methods</li></ol><p><strong>Their innovations:</strong></p><ol><li>Two stage GAN: Low-quality images are generated in stage 1 and parameters frozened to feed stage-2 GAN to generate high resolution images.</li><li>Conditional augmentation: To solve training data deficiency, they calculat mean and variance in a FC layer following the word embedding layer to sample conditional variant c. They also apply KL divergence over the above distribution and normal one(N(0,1)) as regulization. It helps generate more training data and after increasing sampling randomness, one sentence can output different images.</li></ol><p><strong>Implementations:</strong></p><p>TODO</p>]]></content>
    
    
    <categories>
      
      <category>cv</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper-reading</tag>
      
      <tag>cv</tag>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Paper-reading transformer</title>
    <link href="/site/2021/07/07/Transformer/"/>
    <url>/site/2021/07/07/Transformer/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> Attention is all you need</p><p><strong>Task:</strong> sequence to sequence</p><p><strong>Abstracts:</strong> </p><ol><li>a new network replacing recurrent network and convolutions with self-attentions only.</li><li>draw global dependencies between input and output.</li><li>more parallelization and can reach a new state of the art in translation quality in no time.</li></ol><p><strong>Model Architechture:</strong></p><ol><li>Encoder-decoder</li><li>Encoder: an input sequence of symbol representations to a sequence of continuous representations</li><li>Decoder: Given z, the decoder then generates an output sequence of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.</li></ol><p><img src="/site/img/transformer-arch.png"></p><ol start="4"><li>Scaled Dot-Product Attention: We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by √dk.</li><li>Multi-head attention: On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv -dimensional output values. These are concatenated and once again projected, resulting in the final values. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</li><li>Position-wise Feed-Forward Networks:Another way of describing this is as two convolutions with kernel size 1</li><li>Positional encodings: To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. </li></ol><p><strong>Other tricks:</strong></p><ol><li>why self-attention:</li></ol><p>One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. </p><ol start="2"><li><p>dropout and label smoothing:</p><p>Dropout=0.1, label smoothing=0.1</p></li></ol><p><strong>Future works:</strong></p><ol><li>Applications in cv.</li><li>Investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs. </li><li>Making generation less sequential.</li></ol>]]></content>
    
    
    <categories>
      
      <category>Natual Language Processing</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper-reading</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Paper-reading textCNN</title>
    <link href="/site/2021/07/07/textCNN/"/>
    <url>/site/2021/07/07/textCNN/</url>
    
    <content type="html"><![CDATA[<p><strong>Paper:</strong> TextCNN</p><p><strong>Task:</strong> Sentence Classification</p><p><strong>Abstracts:</strong> </p><ol><li>Based on static word embeddings from pretrained models, finetuning with basic CNN models may get good results.</li><li>Finetuning pretrained word embeddings can be useful.</li><li>Making use of both static and task-specific vectors is possible.</li><li>They got SOTAs on 4/7 tasks for sentiment analysis and question classifications.</li></ol><p><strong>Introduction:</strong></p><ol><li>Deep learning</li><li>word vectors</li><li>CNN in NLP(semantic parsing, search query retrieving, and other traditional NLP tasks)</li></ol><p><strong>Model</strong></p><p><img src="/site/img/textcnn1.png"></p><ol><li>k-dimensional word embedding as input(2 channels static and non-static)</li><li>kernel size {3*4*5}*k as different filters, then max pooling(mulple filters to produce multiple feature maps for one word)</li><li>Fc layer with softmax</li></ol><p><strong>Non-static vs static</strong></p><p><img src="/site/img/textcnn2.png"></p><p>Non-static(parameters are updated rather than frozened) is more specific and accurate on tasks.</p><p><strong>Implementations:</strong></p><p>TODO</p>]]></content>
    
    
    <categories>
      
      <category>Natual Language Processing</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper-reading</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/site/2021/06/30/hello-world/"/>
    <url>/site/2021/06/30/hello-world/</url>
    
    <content type="html"><![CDATA[<h2 id="Start-creating-today"><a href="#Start-creating-today" class="headerlink" title="Start creating today!"></a>Start creating today!</h2><p>It’s better late than none.</p><p>The blog name is “Zoom In”, which reminds me to pay attention to the technical details in my career as a machine learning software engineer (for now). I will leave technical notes about artificial intelligence, software engineering and more. In the years to come, I would like to be a <strong>“full-stack” MLE</strong> , skilled in general machine learning, recommendation, computer vision, NLP, reinforcement learning, auto-pilot, and related development skills. Keep it up!!!</p><p>The last but not the least, there will be a “Zoom Out” blog definitely, which focuses on ideas and thoughts in a bigger picture beyond technology. It’s said that the more you get versent in technical skills, the less you think with scope and dimension. Hope it will help me think out of the box as not only an engineer, but also an innovator.</p>]]></content>
    
    
    
    <tags>
      
      <tag>life</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
